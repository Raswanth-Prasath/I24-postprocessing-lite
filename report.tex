% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW versQion
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{subcaption}

% recommended packages for float control & better tables
\usepackage{dblfloatfix}   % improved placement for figure*/table* (works with twocolumn)
\usepackage{placeins}      % \FloatBarrier
\usepackage{caption}       % caption tweaks if needed
\usepackage{siunitx}       % optional: better numeric alignment
\usepackage{multirow}      % optional: for complex tables
% booktabs already included (you have it)

\setlength{\tabcolsep}{3pt}  % default is 6pt


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Vehicle Trajectory Reconstruction using Deep Learning}

\author{Raswanth Prasath\\
Arizona State University\\
% Institution1 address\\
{\tt\small raswanth@asu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Multi-object Tracking (MOT) is important in many applications like visual surveillance, autonomous driving, and behavior analysis. MOT aims to produce individual trajectories from a video sequence to monitor their movements and interactions. MOT faces challenges in associating noisy detections into coherent trajectories, especially under object occlusions and tracking inconsistencies from multiple cameras. This research extends the Negative Cycle Canceling (NCC) algorithm, a network flow-based method that optimizes global data association probability by iteratively resolving conflicting trajectories through efficient negative cycle detection, which enables real-time adjustments to streaming data. The performance of NCC critically depends on its cost function for measuring track fragment similarities. We propose a deep learning architecture that improves the accuracy of fragment association. The model will be trained and evaluated on the I-24 MOTION dataset.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
Vehicle trajectories are foundational data structures used in various transportation applications, including training and testing AI algorithms for autonomous driving, traffic simulation, and safety analysis. However, raw trajectory data extracted from sensor systems are often  fragmented due to occlusions, detection failures, and tracking errors. These fragmentations compromise the quality and utility of trajectory data for downstream applications. This research addresses the data association challenge by using the Negative Cycle Canceling (NCC) algorithm \cite{wang2023online, wang2022automatic}, which formulates the problem as a minimum-cost network circulation task on a graph representing potential fragment connections. The performance of online NCC mainly depends on its cost function, which quantifies the likelihood that two fragments belong to the same vehicle. We propose a deep learning architecture that improves the accuracy of fragment association. 


The first step in trajectory reconstruction is data association of the fragments. For a given set of trajectory fragments we need to identify which fragments originate from the same vehicle and map them into complete trajectories. This problem is typically addressed through graph optimization methods, where fragments are represented as nodes and potential associations as edges with likelihood of association as cost associated. Min-cost flow and Negative Cycle Cancellation (NCC) algorithm helps to find optimal fragment associations. However, the performance of this algorithm mainly depends on the cost function used to measure similarity between fragments. Current trajectory reconstruction pipelines uses the Bhattacharyya distance as the cost functions. While computationally efficient, these hand-crafted features fail to capture the complex dynamics in real-world vehicle behavior, including accelerations, decelerations, and lane changes. 

Recent research shows that deep learning methods have shown promising results in learning features for various computer vision tasks. One-shot classification(siamese network) have proven effective for similarity learning and have been successfully applied to visual object re-identification tasks. Despite these, challenges arise from the nature of geospatial and trajectory data \cite{choudhury2024towards}. The key challenge lies in learning representations that capture the spatio-temporal dynamics of vehicle motion from trajectory fragments of varying lengths and sampling rates.



\textbf{This paper's contributions include the following:} (1) We propose a learned similarity metric based on a Siamese neural network architecture to replace heuristic cost functions in trajectory reconstruction pipelines. Our model uses the long short-term memory (LSTM) recurrence as encoders to capture the temporal dynamics of trajectory fragments and produce discriminative embeddings. (2) We integrate the cost function with the NCC algorithm for fragment association, maintaining the computational efficiency and optimality guarantees of graph-based optimization while improving association accuracy. (3) We conduct experiments on the I-24 MOTION dataset and compare with the probabilistic method as a baseline.


The I-24 MOTION dataset (Figure~\ref{fig:i24_motion_data}) provides 
vehicle trajectory data from a 4-mile stretch of Interstate 24, 
capturing diverse traffic scenarios, including free-flow, slow traffic, 
and congestion conditions.

% Add the figure
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{img/I24 MOTION.png}
    \caption{Time-space diagram of I-24 MOTION data showing fragmented 
    vehicle trajectories. Trajectory interruptions result from occlusions, inter-camera handoffs, and detection failures, illustrating the core data association problem 
    this work addresses.}
    \label{fig:i24_motion_data}
\end{figure}

\section{Related Work}

\subsection{Multi-Object Tracking and Data Association}
Multi-object tracking (MOT) employs a tracking-by-detection paradigm where objects are detected per-frame and associated across time to form trajectories~\cite{bewley2016simple,zhang2008global}. Early methods used probabilistic approaches like Multiple Hypothesis Tracking (MHT)~\cite{reid1979algorithm} and Joint Probabilistic Data Association (JPDA)~\cite{fortmann1983sonar}. Recent surveys~\cite{emami2020machine,rakai2022data} organize methods into measurement-to-track and track-to-track categories, noting a shift from hand-crafted heuristics to learned similarity functions.

\subsection{Network Flow and Graph Optimization}
Zhang et al.~\cite{zhang2008global} pioneered network flow algorithms for MOT, formulating global data association as min-cost flow problems solvable in polynomial time. Wang et al.~\cite{wang2022automatic,wang2023online} employ min-cost circulation where fragments $\phi_i$ are paired nodes in a circulation graph. The MAP problem becomes:
\begin{equation}
\begin{aligned}
\minimize \sum_{i} c_i f_i + \sum_{i} c_i^{en} f_i^{en} + \sum_{i,j} c_{i,j} f_{i,j} + \sum_{i} c_i^{ex} f_i^{ex}
\end{aligned}
\end{equation}
where $c_{i,j} = -\log P(\phi_i|\phi_j)$ represents transition costs. The Negative Cycle Cancellation (NCC) algorithm~\cite{klein1967primal} repeatedly finds negative-cost cycles in residual graphs until optimality. Lenz et al.~\cite{lenz2015followme} introduced memory-bounded online variants for streaming data.

\subsection{Learned Cost Functions for Data Association}
\noindent\textbf{Siamese Networks.} The idea of learning pairwise similarities through Siamese networks has proven particularly effective for tracking tasks. These architectures process pairs of inputs through weight-shared networks, learning to measure similarity in a latent space where matching objects cluster together~\cite{leal2016learning}. Leal-Taixé et al.~\cite{leal2016learning} pioneered this approach for visual tracking, training Siamese CNNs to learn similarity functions $S(x_i, x_j)$ that can be directly converted to assignment costs as $c_{ij} = 1 - S(x_i, x_j)$. While their work focused on image features for pedestrian tracking, we can use the same framework  to kinematic trajectory features, using recurrent encoders to handle variable-length temporal sequences rather than fixed-size images.

\noindent\textbf{LSTM-based Methods.} Temporal dependencies matter enormously in tracking scenarios, and LSTMs have emerged as a natural fit for modeling these sequential patterns.  Milan et al.~\cite{milan2017online} designed LSTM cells outputting assignment probabilities: $A_{t+1}^i = \text{LSTM}(C_{t+1}, h_t, c_t)$. Rakai et al.~\cite{rakai2022data} report growing LSTM popularity (2017-2021), noting improved accuracy and robustness despite increased computational complexity. Recent work~\cite{liu2019deepda,xu2018hierarchical} combines LSTMs with traditional methods, demonstrating superior performance over pure optimization approaches.

\noindent\textbf{Transformer Approaches.} TADN~\cite{psalta2025transformer} uses transformers to directly infer assignments without explicit optimization, while GTR~\cite{wang2024gtr} employs multi-view encoders for trajectory representation. However, transformers require substantial data and computation. For fragment association with limited labeled pairs, LSTM-based Siamese networks provide sample-efficient alternatives.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{img/taxonomy.png}
    \caption{The categories of data association methods.}
    \label{fig:taxonomy}
\end{figure}

% Graser et al.~\cite{graser2023deep} categorize trajectory representations: raw coordinates~\cite{wang2018arrive}, spatial embeddings~\cite{tenzer2022meta}, discretized grids~\cite{musleh2022towards}, and differential encodings~\cite{tritsarolis2021online}. 
Beyond the association mechanism itself, how we represent trajectories fundamentally shapes what patterns a model can learn. For similarity learning, methods like DeepMove~\cite{feng2018deepmove} have shown the power of attention-based GRUs for generating embeddings from location sequences, while VANext~\cite{gao2019predicting} combines GRU and CNN for trajectory representations. These demonstrate learned embeddings outperform hand-crafted features.
% Our Siamese architecture generates fixed-size embeddings from variable-length fragments, focusing on pairwise similarity for association rather than next-location prediction.

\subsection{Vehicle Trajectory Reconstruction}
Wang et al.~\cite{wang2022automatic} presented a comprehensive pipeline for vehicle trajectory reconstruction using the I-24 MOTION dataset, using Bhattacharyya distance for fragment association:
\begin{equation}
c_{i,j}^{Bhat} = -\log \int \sqrt{p_i(x) p_j(x)} dx
\end{equation}
where $p_i$ and $p_j$ are kinematic state distributions. Their method projects fragments forward assuming constant velocity, measuring overlap between predicted and observed positions. While effective, this relies on simplified kinematic models failing during lane changes or aggressive maneuvers. Various interpolation techniques~\cite{montanino2015trajectory,punzo2011speed} handle missing data as post-processing but don't improve association quality.

Our work builds upon the existing pipeline developed for the I-24 MOTION
project. However, it is not straightforward to apply the same strategies to the problem of multitarget tracking for numerous reasons. We learn data-driven similarity metrics capturing complex vehicle dynamics and replace $c_{i,j}^{Bhat}$ with learned costs $c_{i,j}^{learned} = 1 - S_{NN}(\phi_i, \phi_j)$. Further, compared to trackers, we operate on kinematic features rather than images, using LSTM encoders for variable-length temporal sequences. 

% While transformers~\cite{psalta2025transformer,wang2024gtr} offer powerful learning, our LSTM-based architecture provides computational efficiency and sample efficiency, maintaining NCC compatibility while significantly improving association accuracy across diverse traffic scenarios.


\section{Approach}
\label{sec:method}

We formulate trajectory reconstruction as a graph optimization problem where the edge weights are determined by a learned deep similarity metric. The overall system pipeline is illustrated in Figure~\ref{fig:pipeline}.

% \begin{figure*}[t]
%     \centering
%     % Placeholder for Pipeline Figure
%     % Shows: RAW Fragments -> Feature Extraction -> Siamese Network -> Cost Matrix -> NCC Algorithm -> Reconstructed Trajectories
%     \includegraphics[width=0.9\textwidth, height=4cm]{img/Process.png}
%     \caption{System pipeline for learned trajectory reconstruction. The pipeline takes fragmented trajectories as input, extracts kinematic features, computes pairwise similarity scores using a Siamese BiLSTM network, constructs a min-cost circulation graph with learned edge weights, and applies the NCC algorithm to produce complete vehicle trajectories.}
%     \label{fig:pipeline}
% \end{figure*}

\subsection{Problem Formulation}
Let $\mathcal{F} = \{f_1, f_2, \dots, f_N\}$ be a set of trajectory fragments. Each fragment $f_i$ is a sequence of kinematic states $\mathbf{x}_t = [x_t, y_t, v_t, \tau_t]$, representing longitudinal position, lateral position, velocity, and timestamp, respectively. Due to sensor noise, a single vehicle's ground truth path is split into a subset of ordered fragments.

We construct a directed graph $G=(V, E)$, where nodes $V$ represent fragments. An edge $(i, j)$ exists if fragment $f_j$ can plausibly follow $f_i$ (i.e., $f_j$ starts after $f_i$ ends within a defined spatiotemporal window). The objective of the NCC algorithm is to select a subset of edges that minimizes the total cost while satisfying flow conservation constraints. The cost of an edge $c_{i,j}$ should be inversely proportional to the probability that $f_i$ and $f_j$ belong to the same vehicle:
\begin{equation}
    c_{i,j} = -\log P(f_i \to f_j | \text{Same Vehicle})
\end{equation}
Our goal is to approximate this probability using a logistic regression classifier and neural network $S(f_i, f_j; \theta)$.

% \subsection{Data Preprocessing}
% The raw input consists of sequences of varying lengths. To facilitate learning, we normalize the spatial coordinates relative to the start of the first fragment in the pair. Specifically, for a pair $(f_i, f_j)$, we extract sequences of features:
% \begin{equation}
%     \mathbf{u}_t = [\tilde{x}_t, \tilde{y}_t, v_t, \tilde{\tau}_t]
% \end{equation}
% where $\tilde{\tau}_t$ is the time elapsed since the start of the fragment. This relative encoding ensures the model learns motion dynamics (shape and velocity profile) rather than overfitting to absolute geospatial coordinates. Sequences are padded to a fixed maximum length within a batch, and valid lengths are passed to the network to ensure padding does not influence the encoding.

\subsection{Logistic Regression}

First, we model logistic regression to estimate the probability  $p \in [0, 1]$ that two fragments are associated.

\begin{equation}
    p_{i,j} = \sigma(\mathbf{w}^\top \mathbf{x}_{i,j} + b)
\end{equation}
where $\mathbf{x}_{i,j}$ is the engineered feature vector, $\mathbf{w}$ and $b$ are learned parameters, and $\sigma(\cdot)$ is the sigmoid function.  Training minimizes the regularized negative log-likelihood:
\begin{equation}
    \min_{\mathbf{w}, b} -\sum_{(i,j)} \big[y_{i,j}\log p_{i,j} + (1-y_{i,j})\log(1-p_{i,j})\big] + \lambda \lVert \mathbf{w} \rVert_2^2
\end{equation}

We use $\ell_2$ regularization to prevent overfitting and class-balanced. This probability is converted to a
 cost compatible with the Bhattacharyya baseline:

 \begin{equation}
 \text{cost} = (1 - p) \cdot s + \lambda_t \cdot \Delta t
 \end{equation}

 where:
 \begin{itemize}
     \item $p$ = predicted probability of same vehicle (logistic regression) or $S$ (Siamese similarity)
     \item $s = 5$ = scale factor (to match Bhattacharyya cost range)
     \item $\lambda_t = 0.1$ = time penalty weight
     \item $\Delta t$ = time gap between fragments
 \end{itemize}
% The existing Bhattacharyya distance-based cost function uses hand-crafted statistical measures with manually tuned parameters ($c_x$, $m_x$, $c_y$, $m_y$). While this method is effective, it required tuning of parameters. 


\subsubsection{Training Pair Dataset}
We constructed training and test pairs from the raw fragments (RAW-i, RAW-ii, RAW-iii) using ground-truth vehicle IDs from GT-i, GT-ii, and GT-iii. Positive pairs are generated by pairing fragments with same ground truth ids.  pairs must be sequential (no temporal overlap), share the same direction, have time gaps $\leq 5$~s, longitudinal gaps within $[-50, 200]$~ft (direction-aware), and lateral offsets $\leq 5$~ft. Hard negative. We balance the dataset by ensuring equal numbers of negative and positive pairs. 


We evaluated more than 28 features capturing temporal, spatial, kinematic, vehicle, confidence, projection, and curvature characteristics of fragment pairs. To select a compact, interpretable feature for logistic regression, we evaluate the features using the coefficient magnitude, permutation importance, L1‑regularization, and Recursive Feature Elimination (RFE). The logistic regression feature set is listed in Table~\ref{tab:feature_list}.

\begin{table}[t]
  \caption{Logistic regression features and definitions.}
  \label{tab:feature_list}
  \centering
  \footnotesize
  \renewcommand{\arraystretch}{1.05}
  \setlength{\tabcolsep}{4pt} % reduce inter-column padding
  \begin{tabular}{@{} >{\raggedright\arraybackslash}p{0.30\columnwidth}
                    @{\quad} >{\raggedright\arraybackslash}p{0.63\columnwidth} @{}}
  \toprule
  \textbf{Feature} & \textbf{Definition} \\
  \midrule
  y\_diff & Absolute difference between the mean lateral positions of the two fragments. \\[4pt]
  time\_gap & Time gap between fragment A ending and fragment B starting. \\[4pt]
  projection\_error\_x\_max & Maximum absolute longitudinal error when fragment A is linearly projected forward to fragment B's timestamps (from all points in the track). \\[4pt]
  length\_diff & Absolute difference in mean vehicle length between fragments. \\[4pt]
  width\_diff & Absolute difference in mean vehicle width between fragments. \\[4pt]
  projection\_error\_y\_max & Maximum absolute lateral error between projected and observed positions (from all points in the track). \\[4pt]
  bhattacharyya\_coeff & Similarity score derived from Bhattacharyya distance between projected and observed 2D position distributions (higher = more similar). \\[4pt]
  projection\_error\_x\_mean & Mean longitudinal projection error over the first part of fragment B. \\[4pt]
  curvature\_diff & Absolute difference in mean trajectory curvature between fragments. \\[4pt]
  projection\_error\_x\_std & Standard deviation of longitudinal projection error (dispersion of mismatch). \\
  \bottomrule
  \end{tabular}
\end{table}

  
\subsubsection{Training}
We train the logistic regression model with a stratified train/test split and
    five-fold cross-validation. Total pairs: 2,100 (1,050 positive + 1,050 negative pairs), and the train/test split is 80/20. Train: 1,680 pairs (840 positive + 840 negative) and Test: 420 pairs (210 positive + 210 negative).

\subsubsection{Model Evaluation}
Figure~\ref{fig:lr_evaluation} shows the classification performance of the 10-feature logistic regression model on the held-out test set (483 pairs: 241 positive, 242 negative).

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{img/lr_evaluation_results.png}
    \caption{Logistic regression evaluation results: (a) ROC curve showing AUC=0.999, (b) Precision-Recall curve with AP=0.999, (c) Normalized confusion matrix, and (d) Probability score distribution showing clear separation between classes.}
    \label{fig:lr_evaluation}
\end{figure}

The model achieves near-perfect classification with ROC-AUC of 0.999 and average precision of 0.999 on the test set. The confusion matrix shows 237 true negatives, 241 true positives, only 5 false positives, and zero false negatives out of 483 test pairs, resulting in 99.0\% accuracy. The probability distribution demonstrates clear separation between same-vehicle and different-vehicle pairs, validating the discriminative power of the selected features.

\subsubsection{Evaluation Metrics}
 We use CLEAR MOT metrics (Precision, Recall, MOTA, MOTP, Fragmentation, and Switches) to evaluate for trajectory-level quality. Table~\ref{tab:combined_results} reports trajectory-level metrics across three scenarios. Logistic regression improves association quality relative to the raw baseline.

\begin{table*}[t!]
\caption{Comparison of Bhattacharyya and Logistic Regression reconstruction against shared GT and RAW baselines for scenarios i--iii}
\label{tab:combined_results}
\centering
\footnotesize
\renewcommand{\arraystretch}{1}
\begin{tabular}{lcccc|cccc|cccc}
\toprule
\textbf{Metrics} &
GT-i & RAW-i & Bhat-REC-i & LR-REC-i &
GT-ii & RAW-ii & Bhat-REC-ii & LR-REC-ii &
GT-iii & RAW-iii & Bhat-REC-iii & LR-REC-iii \\
\midrule
Precision \(\uparrow\)
& 1.00 & 0.94 & 0.96 & 0.96
& 1.00 & 0.80 & 0.81 & 0.69
& 1.00 & 0.80 & 0.76 & 0.80 \\

Recall \(\uparrow\)
& 1.00 & 0.39 & 0.84 & 0.83
& 1.00 & 0.27 & 0.73 & 0.74
& 1.00 & 0.23 & 0.60 & 0.59 \\

MOTA \(\uparrow\)
& 1.000 & 0.358 & 0.799 & 0.797
& 1.000 & 0.195 & 0.563 & 0.383
& 1.000 & 0.167 & 0.414 & 0.444 \\

MOTP \(\uparrow\)
& 1.000 & 0.582 & 0.644 & 0.644
& 1.000 & 0.649 & 0.673 & 0.668
& 1.000 & 0.668 & 0.683 & 0.688 \\

Fgmt/GT \(\downarrow\)
& 1.00 & 2.52 & 1.46 & 1.52
& 1.00 & 4.15 & 1.08 & 1.69
& 1.00 & 4.18 & 1.34 & 1.32 \\

Sw/GT \(\downarrow\)
& 0.00 & 1.52 & 0.49 & 0.54
& 0.00 & 3.07 & 0.10 & 16.39
& 0.00 & 3.19 & 1.50 & 0.58 \\

No. trajs
& 313 & 789 & 457 & 477
& 99 & 411 & 107 & 167
& 266 & 1112 & 357 & 350 \\
\bottomrule
\end{tabular}
\end{table*}



\subsubsection{Timespace Diagram}

We visualize time-space diagrams
for eastbound (EB) and westbound (WB) directions across
scenarios in figure \ref{fig:timespace_i_EB}, \ref{fig:timespace_i_WB}, \ref{fig:timespace_ii_EB}, \ref{fig:timespace_ii_WB}, \ref{fig:timespace_iii_EB}, and \ref{fig:timespace_iii_WB}.


\begin{figure*}[ht]
    \centering
    \includegraphics[width= 0.825\linewidth]{img/timespace_i_EB.png}
    \caption{scenario i comparison}
    \label{fig:timespace_i_EB}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width= 0.825\linewidth]{img/timespace_i_WB.png}
    \caption{scenario i comparison}
    \label{fig:timespace_i_WB}
\end{figure*}


\begin{figure*}[ht]
    \centering
    \includegraphics[width= 0.825\linewidth]{img/timespace_ii_EB.png}
    \caption{scenario ii comparison}
    \label{fig:timespace_ii_EB}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width= 0.825\linewidth]{img/timespace_ii_WB.png}
    \caption{scenario ii comparison}
    \label{fig:timespace_ii_WB}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width= 0.825\linewidth]{img/timespace_iii_EB.png}
    \caption{scenario iii comparison}
    \label{fig:timespace_iii_EB}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width= 0.825\linewidth]{img/timespace_iii_WB.png}
    \caption{scenario iii comparison}
    \label{fig:timespace_iii_WB}
\end{figure*}